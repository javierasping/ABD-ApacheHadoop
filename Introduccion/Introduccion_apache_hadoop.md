## 1.1 Introducción a Apache Hadoop

En el panorama del procesamiento y análisis de datos a gran escala, Apache Hadoop se ha establecido como una piedra angular fundamental. Este framework de software, desarrollado por Doug Cutting en 2006, surgió como una solución innovadora para abordar el desafío de procesar grandes volúmenes de datos de manera eficiente y rentable.

Hadoop, basado en Java, ofrece una arquitectura distribuida que permite fragmentar tareas de cálculo y distribuirlas en nodos de un clúster de computadoras, lo que permite un procesamiento paralelo de datos. Esta capacidad de escalar horizontalmente, utilizando hardware estándar, ha democratizado el acceso al procesamiento de Big Data, eliminando la necesidad de costosos equipos de última generación.

El núcleo de Hadoop está compuesto por componentes esenciales como Hadoop Common, Hadoop Distributed File System (HDFS) y MapReduce (posteriormente reemplazado por YARN en versiones más recientes). Estos elementos forman la base sobre la cual se construye un ecosistema rico y diverso de herramientas y extensiones, desarrolladas tanto por la comunidad de código abierto como por proveedores de software.

El crecimiento y la evolución de Hadoop han sido impulsados por su adopción generalizada en una variedad de sectores industriales. Desde empresas tecnológicas líderes hasta organizaciones gubernamentales y empresas emergentes, Hadoop se ha convertido en una infraestructura central para el procesamiento y análisis de datos a gran escala.

Junto con su creciente popularidad, han surgido diversas distribuciones profesionales de Hadoop, ofreciendo soporte y servicios adicionales para satisfacer las necesidades específicas de diferentes organizaciones. Ejemplos incluyen Cloudera, Hortonworks, Microsoft Azure y IBM InfoSphere BigInsights, cada uno aportando su experiencia y valor agregado al ecosistema Hadoop.
