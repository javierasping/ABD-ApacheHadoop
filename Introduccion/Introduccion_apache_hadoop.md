## 1.1 Introducción a Apache Hadoop

Debemos entender que Apache Hadoop es un framework de software que aporta la capacidad de ejecutar aplicaciones distribuidas y escalables, generalmente para el sector del Big Data. Así, permite a las aplicaciones hacer uso de miles de nodos de procesamiento y almacenamiento y petabytes de datos.

Hadoop es una de las tecnologías más populares en el ámbito de aplicaciones Big Data. Es usado en multitud de empresas como plataforma central en sus Data Lakes (Lagos de datos), sobre la que se construyen los casos de uso alrededor de la explotación y el almacenamiento de los datos.

Además, es una plataforma sobre la que desarrollar para sacar partido a los datos de las organizaciones, por ejemplo mediante técnicas y modelos de machine learning.

Entre sus principales ventajas se encuentra el hecho de que está diseñado para ejecutar sobre hardware sencillo, normal y corriente. La potencia del sistema se encuentra en que todos los nodos juntos actúan como un clúster con la capacidad de escalar horizontalmente al agregar más nodos.
