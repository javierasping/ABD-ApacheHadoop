## 1.2 Historia

Apache Hadoop surge como respuesta al desafío de procesar la creciente cantidad de datos generados en la World Wide Web a fines de los 90 y principios de los 2000. Inicialmente, la búsqueda de información en la web se basaba en índices creados manualmente, pero con el crecimiento exponencial de la web, se necesitaba automatización. Proyectos como Nutch, concebido por Doug Cutting y Mike Cafarella, buscaban procesar datos de manera distribuida para generar resultados de búsqueda más rápidos. Este concepto se inspiró en los trabajos de Google, que también se centraban en el procesamiento distribuido de datos.

En 2006, Doug Cutting se unió a Yahoo, llevando consigo el proyecto Nutch y la visión de procesamiento distribuido. Así nació Hadoop, con el objetivo de distribuir datos y cálculos en múltiples computadoras para procesar tareas simultáneamente. En 2008, Yahoo liberó Hadoop como proyecto de código abierto. Hoy en día, la Apache Software Foundation (ASF) gestiona y mantiene Hadoop, ofreciéndolo al público como Apache Hadoop desde noviembre de 2012.

El nombre "Hadoop" proviene del elefante de peluche del hijo de Doug Cutting y originalmente fue desarrollado para apoyar el proyecto de motor de búsqueda Nutch. Desde entonces, Hadoop ha evolucionado hasta convertirse en una infraestructura esencial para el procesamiento de datos a gran escala, gracias a su comunidad global de programadores y contribuyentes en la ASF.

En el año 2006, los dos componentes que formaban parte de Hadoop: MapReduce y HDFS se cedieron a la Apache Software Foundation como proyecto open source. Esto impulsó su adopción como herramienta Big Data en proyectos en muchas industrias. El proyecto fue desarrollado en el lenguaje de programación Java.

La versión 1.0 de Hadoop fue publicada en el año 2012. La versión 2.0 se publicó en el año 2013 añadiendo Yarn como gestor de recursos y desacoplando HDFS de MapReduce. En el año 2017 se publicó Hadoop 3.0 añadiendo mejoras.